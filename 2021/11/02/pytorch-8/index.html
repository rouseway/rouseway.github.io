<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="人工智能,Deep Learning,PyTorch," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="下载单标签多分类数据集20 Newsgroups，该数据集包含 20 个不同主题的新闻组文章。我们将在本节请全方位地通过文本分类任务介绍如何进行深度学习实验，包括数据集的构建与划分、训练集与验证集上的调参，以及最终的测试报告生成。 数据预处理我们下载的数据解压后是由20个文件夹组成的，每个文件夹对应一个分类的类别，文件夹的名字就是类别名称，文件夹内存放着该类别的新闻文本，一篇新闻对应一个文本文件。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch实战：文本分类">
<meta property="og:url" content="http://yoursite.com/2021/11/02/pytorch-8/index.html">
<meta property="og:site_name" content="思维驿站">
<meta property="og:description" content="下载单标签多分类数据集20 Newsgroups，该数据集包含 20 个不同主题的新闻组文章。我们将在本节请全方位地通过文本分类任务介绍如何进行深度学习实验，包括数据集的构建与划分、训练集与验证集上的调参，以及最终的测试报告生成。 数据预处理我们下载的数据解压后是由20个文件夹组成的，每个文件夹对应一个分类的类别，文件夹的名字就是类别名称，文件夹内存放着该类别的新闻文本，一篇新闻对应一个文本文件。">
<meta property="article:published_time" content="2021-11-01T16:00:00.000Z">
<meta property="article:modified_time" content="2022-10-02T09:16:47.621Z">
<meta property="article:author" content="rouseway">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2021/11/02/pytorch-8/"/>





  <title> PyTorch实战：文本分类 | 思维驿站 </title>
<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">思维驿站</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">思考的停滞才是真正的懒惰</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/11/02/pytorch-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="rouseway">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://images.weserv.nl/?url=https://article.biliimg.com/bfs/article/cde6d7c5d3ef6c223b7084f947974abd880f42b2.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="思维驿站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                PyTorch实战：文本分类
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-11-02T00:00:00+08:00">
                2021-11-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/" itemprop="url" rel="index">
                    <span itemprop="name">技术文章</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PyTorch/" itemprop="url" rel="index">
                    <span itemprop="name">PyTorch</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>下载单标签多分类数据集<a href="http://qwone.com/~jason/20Newsgroups/" target="_blank" rel="noopener">20 Newsgroups</a>，该数据集包含 20 个不同主题的新闻组文章。我们将在本节请全方位地通过文本分类任务介绍如何进行深度学习实验，包括数据集的构建与划分、训练集与验证集上的调参，以及最终的测试报告生成。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>我们下载的数据解压后是由20个文件夹组成的，每个文件夹对应一个分类的类别，文件夹的名字就是类别名称，文件夹内存放着该类别的新闻文本，一篇新闻对应一个文本文件。我们预处理要做的事情包括：遍历文件，生成文本的词序列与类别标签，构建词典，划分训练、验证与测试集。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os,pickle</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessing</span><span class="params">(max_size=<span class="number">50000</span>, min_freq=<span class="number">1</span>)</span>:</span></span><br><span class="line">    vocab_dic = &#123;&#125;</span><br><span class="line">    label2idx = &#123;&#125;</span><br><span class="line">    datas = []</span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> root,dirs,files <span class="keyword">in</span> os.walk(<span class="string">'./data/Classification/20news-18828/'</span>):</span><br><span class="line">        <span class="keyword">if</span> len(files) &gt; <span class="number">0</span>:</span><br><span class="line">            category = os.path.split(root)[<span class="number">-1</span>]</span><br><span class="line">            label2idx[category] = label2idx.get(category, len(label2idx))  </span><br><span class="line">            <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">                <span class="keyword">with</span> open(os.path.join(root, file), encoding=<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">                    doc = []</span><br><span class="line">                    <span class="keyword">for</span> sline <span class="keyword">in</span> fin:</span><br><span class="line">                        word_lst = sline.strip().split(<span class="string">' '</span>)</span><br><span class="line">                        <span class="keyword">for</span> word <span class="keyword">in</span> word_lst:</span><br><span class="line">                            <span class="keyword">if</span> len(word) &gt; <span class="number">0</span>:</span><br><span class="line">                                word = word.lower()</span><br><span class="line">                                vocab_dic[word] = vocab_dic.get(word, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">                                doc.append(word)</span><br><span class="line">                        doc.append(word)</span><br><span class="line">                    datas.append(doc)</span><br><span class="line">                    labels.append(label2idx[category])</span><br><span class="line">    vocab_list = sorted([_ <span class="keyword">for</span> _ <span class="keyword">in</span> vocab_dic.items() <span class="keyword">if</span> _[<span class="number">1</span>] &gt;= min_freq], </span><br><span class="line">                        key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:max_size]</span><br><span class="line">    vocab_list.insert(<span class="number">0</span>, (<span class="string">'&lt;pad&gt;'</span>, min_freq))</span><br><span class="line">    vocab_list.insert(<span class="number">1</span>, (<span class="string">'&lt;unk&gt;'</span>, min_freq))</span><br><span class="line">    word2idx = &#123;word_count[<span class="number">0</span>]: idx <span class="keyword">for</span> idx, word_count <span class="keyword">in</span> enumerate(vocab_list)&#125;</span><br><span class="line">    idx2word = &#123;idx: word_count[<span class="number">0</span>] <span class="keyword">for</span> idx, word_count <span class="keyword">in</span> enumerate(vocab_list)&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/Classification/vocab.pkl'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        pickle.dump((word2idx, idx2word), fout)</span><br><span class="line">    print(<span class="string">"vocab size is: &#123;&#125;"</span>.format(str(len(word2idx))))</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/Classification/label.pkl'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        pickle.dump(label2idx, fout)</span><br><span class="line">    print(<span class="string">"class number is: &#123;&#125;"</span>.format(len(label2idx)))</span><br><span class="line">    <span class="comment"># 划分训练、验证、测试集</span></span><br><span class="line">    X_train, X_test, Y_train, Y_test = train_test_split(datas, labels, test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    X_train, X_eval, Y_train, Y_eval = train_test_split(X_train, Y_train, test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/Classification/train-eval-corpus.pkl'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        pickle.dump((X_train, Y_train, X_eval, Y_eval), fout)</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/Classification/test-corpus.pkl'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> fout:</span><br><span class="line">        pickle.dump((X_test, Y_test), fout)</span><br><span class="line">    print(<span class="string">"train-corpus size is: &#123;&#125;, eval-corpus size is: &#123;&#125;, test-corpus size is: &#123;&#125;"</span>.format(</span><br><span class="line">        len(X_train), len(X_eval), len(X_test)))</span><br></pre></td></tr></table></figure><br>我们设置 max_size=50000, min_freq=1，先单独执行预处理操作，将预处理生成的数据保存到磁盘上。</p>
<h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2><p>这次的数据读取与加载我们仅通过 DataLoader 来实现，同时我们设置了文章的最大长度，采取了 “长则截断，短则补齐” 的方式将所有的文档统一到一个固定长度：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_loader</span><span class="params">(word2idx, datas, labels, max_len, batch_size)</span>:</span></span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> datas:</span><br><span class="line">        doc_tokens = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(min(max_len, len(doc))):</span><br><span class="line">            doc_tokens.append(word2idx.get(doc[i], word2idx[<span class="string">'&lt;unk&gt;'</span>]))</span><br><span class="line">        data.append(doc_tokens + [<span class="number">0</span>] * max(<span class="number">0</span>, max_len - len(doc_tokens)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> DataSet</span><br><span class="line">    dataset = DataSet.TensorDataset(torch.LongTensor(data).to(device), torch.LongTensor(labels).to(device))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> DataSet.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>由于预处理过程并没有将文档转换为单词编号的序列，所以需要同时传入词典、文本、标签三个参数，并且在初始化时完成单词向编号的映射，对于词典中不包含的词，我们赋值为 ‘\<unk>‘ ，不同于语言模型，我们同时返回了数据和标签两个结果（顺序一一对应）。</p>
<h2 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h2><p>我们定义一个使用卷积神经网络进行文本分类的模型：它包含一个 embedding 层，用于将每个单词编号转换成一个连续空间里的向量；然后是若干个卷积层，用于提取文本的特征；卷积之后的结果需要经过 ReLU 激活，并进行最大池化的降维；最后是一个全连接层用来进行分类，我们把多个卷积操作提取的特征池化后拼接在一起输入到全连接层中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super(TextCNN, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(config.vocab_size, config.embed_size)</span><br><span class="line">        self.convs = nn.ModuleList(</span><br><span class="line">            [nn.Conv2d(<span class="number">1</span>, config.num_filters, (k, config.embed_size)) <span class="keyword">for</span> k <span class="keyword">in</span> config.filter_sizes])</span><br><span class="line">        self.dropout = nn.Dropout(config.dropout)</span><br><span class="line">        self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv_and_pool</span><span class="params">(self, x, conv)</span>:</span></span><br><span class="line">        x = torch.relu(conv(x)).squeeze(<span class="number">3</span>)</span><br><span class="line">        x = torch.max_pool1d(x, x.size(<span class="number">2</span>)).squeeze(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.embedding(x)</span><br><span class="line">        out = out.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        out = torch.cat([self.conv_and_pool(out, conv) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs], <span class="number">1</span>)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></p>
<p>由于多个卷积层基本都是相似的参数结构，所以我们通过一个列表包含来循环构建，并使用了一个新的模块 nn.ModuleList 来存储它们，加入 ModuleList 里面的 module 是会自动注册到整个网络上，同时 module 的 parameters 也会自动添加到整个网络中。</p>
<p>这里我们需要重点解释一下网络中各层的输入输出形状，我们输入模型的 x 的形状是 [ batch_size, seq_len ]，经过 embedding 操作后，变成了 [ batch_size, seq_len, embed_size ]；我们使用的卷积网络为 Conv2d 需要的输入是四阶的，即 [ B, C, H, W ]，所以我们需要在第一轴上插入一个维度，变成 [ batch_size, 1, seq_len, embed_size ]（当然，如果我们选择 Conv1d 则不需要进行形状改造）；每个卷积层的输出是通过 conv_and_pool 函数来实现，其中包括对卷积操作后的 ReLU 激活，这里需要计算一下卷积操作的输出：</p>
<script type="math/tex; mode=display">\begin{cases} height_{out} = (height_{in} - height_{kernel} + 2 \times padding)/stride + 1 \\ width_{out} = (width_{in} - width_{kernel} + 2 \times padding)/stride + 1 \end{cases}</script><p>可以看出，卷积操作的输出尺寸受到卷积核的尺寸、padding、步长等因素的影响，本例中我们将卷积核的宽度 W 设置成了与词向量的维度相同的值，则根据公式可以计算得到卷积输出后的 W 为 1，整个张量的形状为 [ batch_size, num_filters, seq_len-k, 1 ]。因为我们的池化操作 max_pool1d 是一维的，所以我们需要把卷积输出张量的最后一个轴（维度为1）剔除掉。池化后的输出形状就变成了 [ batch_size, num_filters, 1 ]，只要把维度为1的第二个轴剔除掉就可以看作是最初输入的特征表示了。多个卷积层之间的输出需要被连接起来。再经过 dropout 后通过全连接层进行分类。</p>
<h2 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h2><p>启动训练之前，我们需要设置好相关的超参数，我们不妨把这些参数预先定义到一个配置类里：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span><span class="params">(object)</span>:</span></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    max_len = <span class="number">300</span></span><br><span class="line">    num_epochs = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">    vocab_size = <span class="number">50002</span></span><br><span class="line">    embed_size = <span class="number">128</span></span><br><span class="line">    filter_sizes = (<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">    num_filters = <span class="number">256</span></span><br><span class="line">    dropout = <span class="number">0.5</span></span><br><span class="line">    num_classes = <span class="number">20</span></span><br><span class="line">    learning_rate = <span class="number">0.002</span></span><br><span class="line">    require_improvement = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">    log_path = <span class="string">'./log/TextCNN.log'</span></span><br><span class="line">    save_path = <span class="string">'./model/TextCNN.ckpt'</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/Classification/label.pkl'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        label2idx = pickle.load(fin)</span><br><span class="line">        class_list = [<span class="string">''</span>] * len(label2idx)</span><br><span class="line">        <span class="keyword">for</span> label,idx <span class="keyword">in</span> label2idx.items():</span><br><span class="line">            class_list[idx] = label</span><br></pre></td></tr></table></figure></p>
<p>然后，我们开始正式的训练过程，其基本步骤和上一节的语言模型是一样的，不同之处在于我们划分了训练集、验证集和测试集，并在训练了100个 batch 后，分别输出在训练集和验证集上的误差与准确率变化。同时，我们使用 tensorboradX 模块来记录训练过程中各种指标的变化，该模块可以方便可视化：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWrite</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(config, model, train_loader, dev_loader, test_loader)</span>:</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)</span><br><span class="line">    </span><br><span class="line">    total_batch = <span class="number">0</span>  <span class="comment"># 记录进行到多少batch</span></span><br><span class="line">    dev_best_loss = float(<span class="string">'inf'</span>)</span><br><span class="line">    last_improve = <span class="number">0</span>  <span class="comment"># 记录上次验证集loss下降的batch数</span></span><br><span class="line">    flag = <span class="literal">False</span>  <span class="comment"># 记录是否很久没有效果提升</span></span><br><span class="line">    writer = SummaryWriter(log_dir=config.log_path + <span class="string">'/'</span> + time.strftime(<span class="string">'%m-%d_%H.%M'</span>, time.localtime()))</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(config.num_epochs):</span><br><span class="line">        print(<span class="string">'Epoch [&#123;&#125;/&#123;&#125;]'</span>.format(epoch + <span class="number">1</span>, config.num_epochs))</span><br><span class="line">        <span class="keyword">for</span> i, (trains, labels) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">            outputs = model(trains)</span><br><span class="line">            model.zero_grad()</span><br><span class="line">            loss = F.cross_entropy(outputs, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">if</span> total_batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                true = labels.data.cpu()</span><br><span class="line">                predic = torch.max(outputs.data, <span class="number">1</span>)[<span class="number">1</span>].cpu()</span><br><span class="line">                train_acc = metrics.accuracy_score(true, predic)</span><br><span class="line">                dev_acc, dev_loss = evaluate(config, model, dev_loader)</span><br><span class="line">                <span class="keyword">if</span> dev_loss &lt; dev_best_loss:</span><br><span class="line">                    dev_best_loss = dev_loss</span><br><span class="line">                    torch.save(model.state_dict(), config.save_path)</span><br><span class="line">                    improve = <span class="string">'*'</span></span><br><span class="line">                    last_improve = total_batch</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    improve = <span class="string">''</span></span><br><span class="line">                time_dif = get_time_dif(start_time)</span><br><span class="line">                msg = <span class="string">'Iter: &#123;0:&gt;6&#125;,  Train Loss: &#123;1:&gt;5.2&#125;,  Train Acc: &#123;2:&gt;6.2%&#125;,  \</span></span><br><span class="line"><span class="string">                Val Loss: &#123;3:&gt;5.2&#125;,  Val Acc: &#123;4:&gt;6.2%&#125;,  Time: &#123;5&#125; &#123;6&#125;'</span></span><br><span class="line">                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))</span><br><span class="line">                writer.add_scalar(<span class="string">"loss/train"</span>, loss.item(), total_batch)</span><br><span class="line">                writer.add_scalar(<span class="string">"loss/dev"</span>, dev_loss, total_batch)</span><br><span class="line">                writer.add_scalar(<span class="string">"acc/train"</span>, train_acc, total_batch)</span><br><span class="line">                writer.add_scalar(<span class="string">"acc/dev"</span>, dev_acc, total_batch)</span><br><span class="line">                model.train()</span><br><span class="line">            total_batch += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> total_batch - last_improve &gt; config.require_improvement:</span><br><span class="line">                <span class="comment"># 验证集loss超过1000batch没下降，结束训练</span></span><br><span class="line">                print(<span class="string">"No optimization for a long time, auto-stopping..."</span>)</span><br><span class="line">                flag = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    writer.close()</span><br><span class="line">    test(config, model, test_loader)</span><br></pre></td></tr></table></figure></p>
<p>在训练集上实时计算并记录损失与准确率是一般训练过程必须实现的，下面附上如何在验证集上计算相关指标的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(config, model, data_iter, test=False)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    loss_total = <span class="number">0</span></span><br><span class="line">    predict_all = np.array([], dtype=int)</span><br><span class="line">    labels_all = np.array([], dtype=int)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> texts, labels <span class="keyword">in</span> data_iter:</span><br><span class="line">            outputs = model(texts)</span><br><span class="line">            loss = F.cross_entropy(outputs, labels)</span><br><span class="line">            loss_total += loss</span><br><span class="line">            labels = labels.data.cpu().numpy()</span><br><span class="line">            predic = torch.max(outputs.data, <span class="number">1</span>)[<span class="number">1</span>].cpu().numpy()</span><br><span class="line">            labels_all = np.append(labels_all, labels)</span><br><span class="line">            predict_all = np.append(predict_all, predic)</span><br><span class="line"></span><br><span class="line">    acc = metrics.accuracy_score(labels_all, predict_all)</span><br><span class="line">    <span class="keyword">if</span> test:</span><br><span class="line">        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=<span class="number">4</span>)</span><br><span class="line">        confusion = metrics.confusion_matrix(labels_all, predict_all)</span><br><span class="line">        <span class="keyword">return</span> acc, loss_total / len(data_iter), report, confusion</span><br><span class="line">    <span class="keyword">return</span> acc, loss_total / len(data_iter)</span><br></pre></td></tr></table></figure></p>
<p>训练结束后，我们需要在测试集上真正检测模型的效果，我们使用 sklearn 中的 metrics 模块来生成测试报告，得到一个比较全面的评估结果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(config, model, test_iter)</span>:</span></span><br><span class="line">    model.load_state_dict(torch.load(config.save_path))</span><br><span class="line">    model.eval()</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    test_acc, test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=<span class="literal">True</span>)</span><br><span class="line">    msg = <span class="string">'Test Loss: &#123;0:&gt;5.2&#125;,  Test Acc: &#123;1:&gt;6.2%&#125;'</span></span><br><span class="line">    print(msg.format(test_loss, test_acc))</span><br><span class="line">    print(<span class="string">"Precision, Recall and F1-Score..."</span>)</span><br><span class="line">    print(test_report)</span><br><span class="line">    print(<span class="string">"Confusion Matrix..."</span>)</span><br><span class="line">    print(test_confusion)</span><br><span class="line">    time_dif = get_time_dif(start_time)</span><br><span class="line">    print(<span class="string">"Time usage:"</span>, time_dif)</span><br></pre></td></tr></table></figure></p>
<p>我们还需要实现一个初始化网络参数的方法，已经有研究论证了对于 ReLU 激活函数最佳的初始化方式是Kaiming He提出来的初始化值（对于 sigmoid 或 tanh 激活函数最佳的初始化方式是 xavier）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_network</span><span class="params">(model, method=<span class="string">'kaiming'</span>, exclude=<span class="string">'embedding'</span>, seed=<span class="number">123</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> name, w <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> exclude <span class="keyword">not</span> <span class="keyword">in</span> name:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">if</span> method == <span class="string">'xavier'</span>:</span><br><span class="line">                    nn.init.xavier_normal_(w)</span><br><span class="line">                <span class="keyword">elif</span> method == <span class="string">'kaiming'</span>:</span><br><span class="line">                    nn.init.kaiming_normal_(w)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    nn.init.normal_(w)</span><br><span class="line">            <span class="keyword">elif</span> <span class="string">'bias'</span> <span class="keyword">in</span> name:</span><br><span class="line">                nn.init.constant_(w, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<p>最后我们把读取数据与训练过程通过一个 run 函数串联起来：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/Classification/vocab.pkl'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        word2idx, _ = pickle.load(fin)</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/Classification/train-eval-corpus.pkl'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        X_train, Y_train, X_eval, Y_eval = pickle.load(fin)</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/Classification/test-corpus.pkl'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        X_test, Y_test = pickle.load(fin)</span><br><span class="line"></span><br><span class="line">    config = Config()</span><br><span class="line">    model = TextCNN(config).to(device)</span><br><span class="line">    init_network(model)</span><br><span class="line"></span><br><span class="line">    train_loader = create_loader(word2idx, X_train, Y_train, config.batch_size)</span><br><span class="line">    eval_loader = create_loader(word2idx, X_eval, Y_eval, config.max_len, config.batch_size)</span><br><span class="line">    test_loader = create_loader(word2idx, X_test, Y_test, config.max_len, config.batch_size)</span><br><span class="line"></span><br><span class="line">    train(config, model, train_loader, eval_loader, test_loader)</span><br></pre></td></tr></table></figure></p>
<h2 id="其他网络"><a href="#其他网络" class="headerlink" title="其他网络"></a>其他网络</h2><p>除了采用卷积网络以外，我们还可以采用循环网络 LSTM 来实现，这里我们介绍三种 RNN 网络的文本分类模型：RNN、RCNN、RNN+Attention。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config, hidden_size, num_layers, type=<span class="string">'RNN'</span>, attn_hidden=None)</span>:</span></span><br><span class="line">        super(TextRNN, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(config.vocab_size, config.embed_size)</span><br><span class="line">        self.lstm = nn.LSTM(config.embed_size, hidden_size, num_layers, bidirectional=<span class="literal">True</span>,</span><br><span class="line">                            batch_first=<span class="literal">True</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line">        self.type = type</span><br><span class="line">        <span class="keyword">if</span> self.type == <span class="string">'RNN'</span>:</span><br><span class="line">            self.fc = nn.Linear(hidden_size * <span class="number">2</span>, config.num_classes)</span><br><span class="line">        <span class="keyword">if</span> self.type == <span class="string">'RCNN'</span>:</span><br><span class="line">            self.maxpool = nn.MaxPool1d(config.max_len)</span><br><span class="line">            self.fc = nn.Linear(hidden_size*<span class="number">2</span>+config.embed_size, config.num_classes)</span><br><span class="line">        <span class="keyword">elif</span> self.type == <span class="string">'Attention'</span>:</span><br><span class="line">            self.tanh1 = nn.Tanh()</span><br><span class="line">            self.w = nn.Parameter(torch.zeros(hidden_size*<span class="number">2</span>))</span><br><span class="line">            self.tanh2 = nn.Tanh()</span><br><span class="line">            self.fc1 = nn.Linear(hidden_size*<span class="number">2</span>, attn_hidden)</span><br><span class="line">            self.fc = nn.Linear(attn_hidden, config.num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        embed = self.embedding(x)</span><br><span class="line">        out, _ = self.lstm(embed)</span><br><span class="line">        <span class="keyword">if</span> self.type == <span class="string">'RNN'</span>:</span><br><span class="line">            out = self.fc(out[:, <span class="number">-1</span>, :])</span><br><span class="line">        <span class="keyword">elif</span> self.type == <span class="string">'RCNN'</span>:</span><br><span class="line">            out = torch.cat((embed, out), <span class="number">2</span>)</span><br><span class="line">            out = F.relu(out)</span><br><span class="line">            out = out.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">            out = self.maxpool(out).squeeze()</span><br><span class="line">            out = self.fc(out)</span><br><span class="line">        <span class="keyword">elif</span> self.type == <span class="string">'Attention'</span>:</span><br><span class="line">            M = self.tanh1(out)</span><br><span class="line">            alpha = F.softmax(torch.matmul(M, self.w), dim=<span class="number">1</span>).unsqueeze(<span class="number">-1</span>)</span><br><span class="line">            out = out * alpha</span><br><span class="line">            out = torch.sum(out, <span class="number">1</span>)</span><br><span class="line">            out = F.relu(out)</span><br><span class="line">            out = self.fc1(out)</span><br><span class="line">            out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></p>
<p>我们还是以网络层的输入输出形状来介绍这三种框架：</p>
<ul>
<li>输入数据的形状： [batch_size, seq_len]</li>
<li>经过 embedding 输出： [batch_size, seq_len, embed_size]</li>
<li>双向 LSTM 输出： [batch_szie, seq_len, hidden_size*2]</li>
<li>三种框架：<ul>
<li><strong>RNN</strong>：<ul>
<li>取最后时刻的隐层值： [batch_size, hidden_size*2]</li>
</ul>
</li>
<li><strong>RCNN</strong>：<ul>
<li>将 embeeding 层与 LSTM 输出拼接，并进行非线性激活： [batch_size, seq_len, hidden_size*2+embed_size]</li>
<li>池化层，seq 个特征中取最大： [batch_size, hidden_size*2+embed_size]</li>
</ul>
</li>
<li><strong>RNN+Attention</strong>：<ul>
<li>初始化一个可学习的权重矩阵 W： [hidden_size*2, 1]</li>
<li>对 LSTM 的输出进行非线性激活后与 W 进行矩阵相乘，并进行 softmax 归一化： [batch_size, seq_len, 1]</li>
<li>将 LSTM 的每一时刻的隐层状态乘对应的注意力分值后求和，得到加权平均后的终极隐层值： [batch_size, hidden_size*2]</li>
</ul>
</li>
</ul>
</li>
<li>全连接： [batch_size, num_class]</li>
<li>Softmax 预测： [batch_size, 1]</li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/10/08/pytorch-7/" rel="next" title="PyTorch实战：语言模型">
                <i class="fa fa-chevron-left"></i> PyTorch实战：语言模型
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/12/17/pytorch-9/" rel="prev" title="PyTorch实战：机器翻译">
                PyTorch实战：机器翻译 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://images.weserv.nl/?url=https://article.biliimg.com/bfs/article/cde6d7c5d3ef6c223b7084f947974abd880f42b2.jpg"
               alt="rouseway" />
          <p class="site-author-name" itemprop="name">rouseway</p>
           
              <p class="site-description motion-element" itemprop="description">在这里留下一些思考的痕迹</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">32</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">18</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据预处理"><span class="nav-number">1.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据加载"><span class="nav-number">2.</span> <span class="nav-text">数据加载</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#定义网络"><span class="nav-number">3.</span> <span class="nav-text">定义网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#开始训练"><span class="nav-number">4.</span> <span class="nav-text">开始训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他网络"><span class="nav-number">5.</span> <span class="nav-text">其他网络</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">rouseway</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next" target="_blank" rel="noopener">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  





  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

</body>
</html>
